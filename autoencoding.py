# -*- coding: utf-8 -*-
"""autoencokmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EVnPWstR7R0ZHJ5hLuOk6lvFnGaAGUQv
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn import metrics
from sklearn.cluster import KMeans
#import util_mnist_reader
from sklearn.model_selection import train_test_split
from tensorflow import keras
from keras.datasets import mnist
#!pip install coclust
from coclust.evaluation.external import accuracy
from sklearn.metrics.cluster import normalized_mutual_info_score as nmi
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, UpSampling2D, Activation
from keras import backend as K
import seaborn as sns
from keras import callbacks
from keras.optimizers import SGD
from keras.initializers import VarianceScaling
from keras.engine.topology import Layer, InputSpec
import keras
import gzip
# %matplotlib inline
from keras.models import Model
from keras.optimizers import RMSprop
from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose
from keras.layers.normalization import BatchNormalization
from keras.models import Model,Sequential
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adadelta, RMSprop,SGD,Adam
from keras import regularizers
from keras import backend as K
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score, normalized_mutual_info_score,confusion_matrix

"""PART 2 - AUTO-ENCODING KMEANS"""

#loading the data
(X_train, y_train), (X_data, y_data) = keras.datasets.fashion_mnist.load_data()

#splitting the data
X_validation, X_test, y_validation, y_test = train_test_split(X_data, y_data, test_size=0.50, shuffle=False)

#setting hyper-parameters
batch_size = 64
epochs = 200
inChannel = 1
x, y = 28, 28
image_input = Input(shape = (x, y, inChannel))
no_of_classes = 10

#normalizing the data
X_train = X_train.reshape(-1, 28,28, 1)/255
X_test = X_test.reshape(-1, 28,28, 1)/255
X_validation = X_validation.reshape(-1, 28,28, 1)/255

def encoder(image_input):
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)
    conv1 = BatchNormalization()(conv1)
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) 
    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) 
    conv2 = BatchNormalization()(conv2)
    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) 
    conv3 = BatchNormalization()(conv3)
    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
    conv3 = BatchNormalization()(conv3)
    conv4 = Conv2D(7, (3, 3), activation='relu', padding='same')(conv3)
    conv4 = BatchNormalization()(conv4)
    conv4 = Conv2D(7, (3, 3), activation='relu', padding='same')(conv4)
    conv4 = BatchNormalization()(conv4)
    return conv4

def decoder(conv4):
    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) 
    conv5 = BatchNormalization()(conv5)
    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)
    conv5 = BatchNormalization()(conv5)
    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) 
    conv6 = BatchNormalization()(conv6)
    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)
    conv6 = BatchNormalization()(conv6)
    up1 = UpSampling2D((2,2))(conv6) 
    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) 
    conv7 = BatchNormalization()(conv7)
    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)
    conv7 = BatchNormalization()(conv7)
    up2 = UpSampling2D((2,2))(conv7)
    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)
    return decoded

autoencoder = Model(image_input, decoder(encoder(image_input)))
autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())

autoencoder.summary()

autoencoder_train = autoencoder.fit(X_train,X_train, batch_size=batch_size,epochs=30,verbose=1,validation_data=(X_validation, X_validation))

loss = autoencoder_train.history['loss']
val_loss = autoencoder_train.history['val_loss']
epochs = 30

#graph plotting
plt.plot(range(0,epochs),loss)
plt.plot(range(0,epochs),val_loss)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Training set","Validation set"])
plt.show()

autoencoder.save_weights('autoencoder.h5')

weights = autoencoder.get_weights()

encode = Model(image_input,encoder(image_input))

encode.summary()

for l1,l2 in zip(encode.layers[:19],autoencoder.layers[0:19]):
    l1.set_weights(l2.get_weights())

pred_encode = encode.predict(X_test)

kmeans_clustering = KMeans(n_clusters=10)

pred_encode = pred_encode.reshape(-1,7*7*7)

clustered_training_set = kmeans_clustering.fit_predict(pred_encode)

#!pip install coclust

from coclust.evaluation.external import accuracy

accuracy(y_test, clustered_training_set)

#confusion matrix

print(confusion_matrix(y_test, clustered_training_set))

from sklearn.utils.linear_assignment_ import linear_assignment
import numpy as np

def _make_cost_m(cm):
    s = np.max(cm)
    return (- cm + s)

indexes = linear_assignment(_make_cost_m(confusion_matrix(y_test, clustered_training_set)))
js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]
cm2 = confusion_matrix(y_test, clustered_training_set)[:, js]
print(cm2)

metrics.adjusted_rand_score(y_test, clustered_training_set)

metrics.normalized_mutual_info_score(y_test,clustered_training_set)

#!pip install sklearn

"""PART 3 Auto-Encoding On GMM"""

from sklearn import mixture

gmm = mixture.GaussianMixture(n_components=10).fit(pred_encode)
labels_gmm = gmm.predict(pred_encode)

accuracy(y_test,labels_gmm)

print(confusion_matrix(y_test, labels_gmm))

from sklearn.utils.linear_assignment_ import linear_assignment
import numpy as np

def _make_cost_m(cm):
    s = np.max(cm)
    return (- cm + s)

indexes = linear_assignment(_make_cost_m(confusion_matrix(y_test, labels_gmm)))
js = [e[1] for e in sorted(indexes, key=lambda x: x[0])]
cm2 = confusion_matrix(y_test, labels_gmm)[:, js]
print(cm2)